# CapStone Design2 

## Path prediction during a tennis match

### Index

#### Data Introduction

#### Code

#### Specific code description

ğŸ—‚ï¸ Data

* ë°ì´í„°ëŠ” ìµœê·¼ ì˜¬ë¦¼í”½ ê²½ê¸°ì˜€ë˜ í…Œë‹ˆìŠ¤ ë‚¨ìë‹¨ì‹ 2íšŒì „ ë…¸ë°• ì¡°ì½”ë¹„ì¹˜ vs ë¼íŒŒì—˜ ë‚˜ë‹¬ì˜ ê²½ê¸° ì¤‘ ì¼ë¶€ë¶„ì„ ë”°ì™”ìŠµë‹ˆë‹¤!

[2024 íŒŒë¦¬ ì˜¬ë¦¼í”½ í…Œë‹ˆìŠ¤ ë‚¨ìë‹¨ì‹](https://www.youtube.com/watch?v=8Mlg7s6gW-M)

ğŸ¾ ì´ ê²½ê¸° ì¤‘ 13:29 ~ 13:54 ê²½ê¸°ë¥¼ ê°€ì§€ê³  ì™”ìŠµë‹ˆë‹¤!

ğŸ’» Code

'''python 

import cv2
import os

video_path = r'D:/Tennis_Video/Tennis_MP4_5.mp4'
output_folder = r'D:/Tennis_Video/Frames'

if not os.path.exists(output_folder):
    os.makedirs(output_folder)

cap = cv2.VideoCapture(video_path)

frame_count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    
    frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')
    cv2.imwrite(frame_filename, frame)
    
    frame_count += 1

cap.release()
print(f"Total frames saved: {frame_count}")
'''

* ì´ ì½”ë“œëŠ” ë½‘ì•„ì˜¨ ì˜ìƒì„ ì½ê³ , ê° í”„ë ˆì„ ë³„ë¡œ ì˜ë¼ JPG í˜•íƒœë¡œ output_folderë¡œ ì €ì¥ë  ìˆ˜ ìˆê²Œë” í•´ë†“ì•˜ìŠµë‹ˆë‹¤.


'''python
import cv2
import os

video_path = 'D:/Tennis_Video/Tennis_MP4_5.mp4'
output_video_path = 'D:/Tennis_Video/Tennis_Output_with_Frame_Label.mp4'
frame_label_dir = 'D:/Tennis_Video/frame_label'

class_mapping = {0: "ball", 1: "player", 2: "tennis racket", 3: "referee"}
class_colors = {
    0: (255, 0, 0),    # Blue for ball
    1: (0, 255, 0),    # Green for player
    2: (0, 0, 255),    # Red for tennis racket
    3: (255, 255, 0)   # Yellow for referee
}

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

frame_index = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    label_file = os.path.join(frame_label_dir, f'frame_{frame_index:04d}.txt')

    if os.path.exists(label_file):
        # Read the label file
        with open(label_file, 'r') as f:
            lines = f.readlines()

        for line in lines:
            values = line.strip().split()
            class_id = int(values[0])
            x_center, y_center = float(values[1]) * width, float(values[2]) * height
            box_width, box_height = float(values[3]) * width, float(values[4]) * height

            x1 = int(x_center - box_width / 2)
            y1 = int(y_center - box_height / 2)
            x2 = int(x_center + box_width / 2)
            y2 = int(y_center + box_height / 2)

            color = class_colors.get(class_id, (255, 255, 255))  
            label = class_mapping.get(class_id, "Unknown")
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

    out.write(frame)
    frame_index += 1

cap.release()
out.release()
cv2.destroyAllWindows()
print("Video processing with frame labels completed.")
'''

* ì´ ì½”ë“œëŠ” ê°ì²´ë¥¼ íƒì§€í•˜ê³  ì˜ìƒì—ì„œ ê·¸ ê°ì²´ë¥¼ íƒì§€í•˜ê³  ìˆìŒì„ ë³´ì—¬ì£¼ê³  ìˆëŠ” ì½”ë“œ ì…ë‹ˆë‹¤.

1. ê²½ë¡œ ë° í´ë˜ìŠ¤ ë§¤í•‘ ì •ì˜: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ, ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ, í”„ë ˆì„ ë¼ë²¨ ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì„¤ì •í•˜ê³ . ë˜í•œ, í´ë˜ìŠ¤ IDì™€ í•´ë‹¹ í´ë˜ìŠ¤ ì´ë¦„ ë° ìƒ‰ìƒì— ë”°ë¼ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

2. ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ì‘ì„±ê¸° ì„¤ì •: ë¹„ë””ì˜¤ íŒŒì¼ì„ ì½ê¸° ìœ„í•´ OpenCVì˜ VideoCapture ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , ë¹„ë””ì˜¤ì˜ ë„ˆë¹„, ë†’ì´, í”„ë ˆì„ ì†ë„ë¥¼ ê°€ì ¸ì˜¤ê³  ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.

3. í”„ë ˆì„ ì²˜ë¦¬: ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—´ë ¤ ìˆëŠ” ë™ì•ˆ ê° í”„ë ˆì„ì„ ì½ì–´ì™€, ê° í”„ë ˆì„ì— ëŒ€í•´ í•´ë‹¹ í”„ë ˆì„ì˜ ë¼ë²¨ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì¡´ì¬í•˜ë©´ ë¼ë²¨ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤. ë¼ë²¨ íŒŒì¼ì—ëŠ” ê°ì²´ì˜ í´ë˜ìŠ¤ IDì™€ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

4. ë°”ìš´ë”© ë°•ìŠ¤ ë° ë¼ë²¨ ê·¸ë¦¬ê¸°: ê° ê°ì²´ì— ëŒ€í•´ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê³„ì‚°í•˜ê³ , í•´ë‹¹ í´ë˜ìŠ¤ì˜ ìƒ‰ìƒê³¼ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì—¬ í”„ë ˆì„ì— ê·¸ë¦½ë‹ˆë‹¤.

5. ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ì— í”„ë ˆì„ ì“°ê¸°: ì²˜ë¦¬ëœ ê° í”„ë ˆì„ì„ ì¶œë ¥ ë¹„ë””ì˜¤ íŒŒì¼ì— ì”ë‹ˆë‹¤.

6. ì‹¤í–‰ ì¢…ë£Œ: ë¹„ë””ì˜¤ ìº¡ì²˜ ë° ì‘ì„±ê¸° ê°ì²´ë¥¼ í•´ì œí•˜ê³ , ëª¨ë“  OpenCV ì°½ì„ ë‹«ìŠµë‹ˆë‹¤.


'''python
import cv2
import os

video_path = 'D:/Tennis_Video/Tennis_MP4_5.mp4'
output_video_path = 'D:/Tennis_Video/Predict_path_Tennis.mp4'
frame_label_dir = 'D:/Tennis_Video/frame_label'

class_mapping = {0: "ball", 1: "player", 2: "tennis racket", 3: "referee"}
class_colors = {
    0: (255, 0, 0),    # Blue for ball
    1: (0, 255, 0),    # Green for player
    2: (0, 0, 255),    # Red for tennis racket
    3: (255, 255, 0)   # Yellow for referee
}

opponent_court_bounds = [(733, 374), (1221, 372), (692, 483), (1266, 487)]

def calculate_distance(point1, point2):
    return ((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)**0.5

def find_optimal_targets(opponent_player, court_bounds):
    distances = [(calculate_distance(opponent_player, corner), corner) for corner in court_bounds]
    distances.sort(reverse=True, key=lambda x: x[0])
    return [corner for _, corner in distances[:3]]

def bounding_boxes_overlap(box1, box2):
    """Check if two bounding boxes overlap."""
    x1_min, y1_min, x1_max, y1_max = box1
    x2_min, y2_min, x2_max, y2_max = box2
    return not (x1_max < x2_min or x2_max < x1_min or y1_max < y2_min or y2_max < y1_min)

def draw_arrows(frame, ball_position, targets):
    for target in targets:
        pt1 = (int(ball_position[0]), int(ball_position[1]))
        pt2 = (int(target[0]), int(target[1]))
        cv2.arrowedLine(frame, pt1, pt2, (0, 0, 255), 2, tipLength=0.2)  
        cv2.circle(frame, pt2, 5, (0, 0, 255), -1) 

cap = cv2.VideoCapture(video_path)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

frame_index = 0

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    label_file = os.path.join(frame_label_dir, f'frame_{frame_index:04d}.txt')

    ball_position = None
    racket_position = None
    players = []

    if os.path.exists(label_file):
        
        with open(label_file, 'r') as f:
            lines = f.readlines()

        for line in lines:
            values = line.strip().split()
            class_id = int(values[0])
            x_center, y_center = float(values[1]) * width, float(values[2]) * height
            box_width, box_height = float(values[3]) * width, float(values[4]) * height

            x1 = int(x_center - box_width / 2)
            y1 = int(y_center - box_height / 2)
            x2 = int(x_center + box_width / 2)
            y2 = int(y_center + box_height / 2)

            if class_id == 0:  
                ball_position = (x_center, y_center)
                ball_box = (x1, y1, x2, y2)
            elif class_id == 2:  
                racket_position = (x_center, y_center)
                racket_box = (x1, y1, x2, y2)
            elif class_id == 1:  
                players.append((x_center, y_center))

            color = class_colors.get(class_id, (255, 255, 255))  
            label = class_mapping.get(class_id, "Unknown")
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

    if ball_position and racket_position and bounding_boxes_overlap(ball_box, racket_box):
        # Find the opponent player farthest from the racket
        if players:
            opponent_player = max(players, key=lambda p: calculate_distance(racket_position, p))
            # Predict target positions
            targets = find_optimal_targets(opponent_player, opponent_court_bounds)
            # Draw arrows
            draw_arrows(frame, ball_position, targets)

    out.write(frame)
    frame_index += 1

cap.release()
out.release()
cv2.destroyAllWindows()
print("Video processing with predictions completed.")
'''

ë¨¼ì € ì´ ì½”ë“œì˜ ê¸°ëŠ¥ì„ ì•Œë ¤ë“œë¦¬ìë©´,

1. ê°ì²´ íƒì§€ ë° ì£¼ì„ í‘œì‹œ
- ê° í”„ë ˆì„ì—ì„œ ê³µ, ì„ ìˆ˜, í…Œë‹ˆìŠ¤ ë¼ì¼“, ì‹¬íŒ ë“±ì˜ ê°ì²´ë¥¼ íƒì§€í•˜ê³  ì£¼ì„ì„ í‘œì‹œí•©ë‹ˆë‹¤.
- ê°ì²´ë¥¼ ì‰½ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„í•˜ê²Œ í•©ë‹ˆë‹¤.

- ê³µ: íŒŒë€ìƒ‰ , ì„ ìˆ˜: ì´ˆë¡ìƒ‰, ë¼ì¼“: ë¹¨ê°„ìƒ‰, ì‹¬íŒ: ë…¸ë€ìƒ‰

2. ê³µ ê¶¤ì  ì˜ˆì¸¡

- ê³µê³¼ ë¼ì¼“ì´ ê²¹ì¹˜ëŠ” ê²½ìš°(íƒ€ê²©ì´ ì´ë£¨ì–´ì§ˆ ê°€ëŠ¥ì„±)ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
- ìƒëŒ€ ì„ ìˆ˜ì˜ ìœ„ì¹˜ì™€ ì½”íŠ¸ ëª¨ì–‘ì„ ê¸°ë°˜ìœ¼ë¡œ ê³µì˜ ë¯¸ë˜ ê¶¤ì ì„ ì˜ˆì¸¡í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.

3. ë™ì  ì‹œê°í™”

- ì˜ˆì¸¡ëœ ê³µì˜ ê¶¤ì ì„ í™”ì‚´í‘œë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
- ëª©í‘œ ì§€ì ì„ ì‘ì€ ë¹¨ê°„ìƒ‰ ì›ìœ¼ë¡œ í‘œì‹œí•˜ì—¬ ëª…í™•ì„±ì„ ë†’ì…ë‹ˆë‹¤.

